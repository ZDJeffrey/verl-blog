{"pageProps":{"posts":[{"slug":"2025-06-07-verl-v0-4","frontmatter":{"title":"VERL 0.4.0: 大型MoE模型、工具调用与低资源友好的强化学习框架","author":"The veRL Team","date":"June 7, 2025","previewImg":"/images/blog/verl_v0_4/cover.png"},"content":"\n火山引擎强化学习框架VERL (Volcano Engine Reinforcement Learning for LLMs)于近日发布了0.4.0版本，带来了一系列重要的新功能和改进。本文将介绍这个版本的亮点功能和主要变化。\n\n## 目录\n- [大型MoE模型支持：DeepSeek 671B与Qwen3 235B](#大型moe模型支持deepseek-671b与qwen3-235b)\n- [工具调用与多轮RL](#工具调用与多轮rl)\n- [低资源友好](#低资源友好)\n- [新算法与配方](#新算法与配方)\n- [FSDP2与训练优化](#fsdp2与训练优化)\n- [部署与硬件支持](#部署与硬件支持)\n- [总结](#总结)\n\n\n## 大型MoE模型支持：DeepSeek 671B与Qwen3 235B\n##### 核心突破\n- **预览功能**：使用Megatron后端支持大型MoE模型的强化学习训练\n- **支持模型**：DeepSeek-V3、Qwen3-235B、Mixtral、Moonlight等\n- **并行技术**：支持专家并行、上下文并行、梯度检查点\n- **分布式检查点**：支持dist-ckpt功能\n这一突破使得训练和微调超大规模混合专家模型成为可能，为研究人员和开发者提供了处理更大模型的能力。\n\n\n## 工具调用与多轮RL\n##### 主要特性\n- **样本级工具调用**：通过SGLang支持工具调用和多轮强化学习\n- **Search-R1配方**：基于此功能构建的搜索增强推理模型训练配方\n- **异步工具调用原型**：通过vLLM AsyncLLM服务器实现\n- **SGLang优化**：支持多节点和多模态\n- **沙盒融合**：集成了安全沙盒功能\n这些功能极大地增强了VERL在训练能够使用工具和进行多轮对话的智能助手方面的能力，为开发复杂的AI代理提供了基础。\n\n\n## 低资源友好\n##### 关键改进\n- **LoRA支持**：使单节点8卡A100上训练70B+模型成为可能\n- **融合交叉熵内核**：大幅降低峰值内存占用 \n  - 通过设置 actor_rollout_ref.model.use_fused_kernels=True 启用\n这些优化使得研究人员和小型团队能够在有限的计算资源上进行大型模型的强化学习训练，降低了入门门槛。\n\n\n## 新算法与配方\n##### 算法创新\n- **PPO和GRPO文档**：详细解释这两种核心算法\n- **DAPO**：解耦裁剪和动态采样策略优化\n- **SPIN**：自对弈微调\n- **SPPO**：自对弈偏好优化\n- **OPO**：具有最优奖励基线的在策略RL\n- **DrGRPO、REINFORCE++、Dual-Clip PPO**等新算法\n##### 新模型与训练工具\n- **Kimi-VL示例**：视觉语言模型支持\n- **Qwen3示例**：最新的通义千问模型支持\n- **视频输入支持**：多模态能力扩展\n- **Warmup-Stable-Decay调度器**：改进的学习率调度\n- **RoPE缩放**：位置编码优化\n- **GPQA和livecodebench评估**：新的评估基准\n- **ClearML日志**：新的实验跟踪选项\n\n\n## FSDP2与训练优化\n##### 架构改进\n- **FSDP2推荐**：取代FSDP1，提供更好的吞吐量和内存使用，并与其他功能（如torch.compile）兼容\n    ```bash\n    actor_rollout_ref.ref.strategy=fsdp2\n    actor_rollout_ref.actor.strategy=fsdp2\n    critic.strategy=fsdp2 \n    reward_model.strategy=fsdp2 \n    ```\n- **CPU卸载**：FSDP2的CPU卸载与梯度累积兼容\n  - 通过`actor_rollout_ref.actor.offload_policy=True`开启以节省内存\n##### 其他优化\n- **激活卸载**：降低内存使用\n- **Ulysses序列并行**：用于视觉语言模型\n- **PPO训练器中的奖励计算优化**：在计算log_prob时同时计算奖励\n- **Ray分析时间线**：用于性能分析\n\n## 部署与硬件支持\n- **dstack简易部署**：简化部署流程\n- **非NVIDIA GPU增强**：改进对AMD等硬件的支持\n\n## 总结\nVERL 0.4.0版本带来了全方位的升级，从支持超大规模MoE模型，到工具调用和多轮对话能力，再到对资源受限环境的优化。这些改进使VERL成为目前最强大、最灵活的大语言模型强化学习框架之一，为研究人员和开发者提供了先进的工具来训练和优化下一代AI系统。\n无论是学术研究还是工业应用，VERL 0.4.0都提供了丰富的功能和工具，帮助用户更高效地训练和部署符合特定需求的大型语言模型。\n","date":1749254400000}]},"__N_SSG":true}